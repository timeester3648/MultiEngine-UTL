// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ DmitriBogdanov/prototyping_utils ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//
// Module:        utl::parallel
// Documentation: https://github.com/DmitriBogdanov/prototyping_utils/blob/master/docs/module_parallel.md
// Source repo:   https://github.com/DmitriBogdanov/prototyping_utils
//
// This project is licensed under the MIT License
//
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#if !defined(UTL_PICK_MODULES) || defined(UTLMODULE_PARALLEL)
#ifndef UTLHEADERGUARD_PARALLEL
#define UTLHEADERGUARD_PARALLEL

// _______________________ INCLUDES _______________________

#include <condition_variable> // condition_variable
#include <cstddef>            // size_t
#include <functional>         // bind()
#include <future>             // future<>, packaged_task<>
#include <mutex>              // mutex, recursive_mutex, lock_guard<>, unique_lock<>
#include <queue>              // queue<>
#include <thread>             // thread
#include <type_traits>        // decay_t<>, invoke_result_t<>
#include <utility>            // forward<>()
#include <vector>             // vector

// ____________________ DEVELOPER DOCS ____________________

// In C++20 'std::jthread' can be used to simplify code a bit, no reason not to do so.
//
// In C++20 '_unroll<>()' template can be improved to take index as a template-lambda-explicit-argument
// rather than a regular arg, ensuring its constexpr'ness. This may lead to a slight performance boost
// as truly manual unrolling seems to be slightly faster than automatic one.

// ____________________ IMPLEMENTATION ____________________

namespace utl::parallel {

// =============
// --- Utils ---
// =============

inline std::size_t max_thread_count() {
    const std::size_t detected_threads = std::thread::hardware_concurrency();
    return detected_threads ? detected_threads : 1;
    // 'hardware_concurrency()' returns '0' if it can't determine the number of threads,
    // in this case we reasonably assume there is a single thread available
}

// No reason to include the entirety of <algorithm> just for 2 one-liner functions,
// so we implement 'std::size_t' min/max here
constexpr std::size_t _min_size(std::size_t a, std::size_t b) noexcept { return (b < a) ? b : a; }
constexpr std::size_t _max_size(std::size_t a, std::size_t b) noexcept { return (b < a) ? a : b; }

// We REALLY don't want compiler to mess up inlining of binary operations or unrolling template
// since that alone can tank the performance so we force inlining just to be sure
#if defined(_MSC_VER) || defined(_MSC_FULL_VER)
#define utl_parallel_force_inline __forceinline
#elif defined(__GNUC__) || defined(__clang__) || defined(__INTEL_COMPILER)
#define utl_parallel_force_inline __attribute__((always_inline))
#else
#define utl_parallel_force_inline
#endif

// Template for automatic loop unrolling.
//
// It is used in 'parallel::reduce()' to speed up a tight loop while
// leaving the used with ability to easily control that unrolling.
//
// Benchmarks indicate speedups ~130% to ~400% depending on CPU, compiler and options
// large unrolling (32) seems to be the best on benchmarks, however it may bloat the binary
// and take over too many branch predictor slots in case of min/max reductions, 4-8 seems
// like a reasonable sweetspot for most machines. We use 4 as a less intrusive option.
//
// One may think that it is a job of compiler to perform such optimizations, yet even with
// GCC '-Ofast -funroll-all-loop' and GCC unroll pragmas it fails to do them reliably.
//
// The reason it fails to do so is pretty clear for '-O2' and below - strictly speaking, most binary
// operations on floats are non-commutative (sum depends on the order of addition, for example), however
// since reduction is inherently not-order-preserving there is no harm in reordering operations some more
// and unrolling the loop so compiler will be able to use SIMD if sees it as possile (it often does).
//
// Why unrolling still fails with '-Ofast' which reduce conformance and allows reordering
// of math operations is unclear, but it is how it happens when actually measured.
//
template <class T, T... indeces, class F>
utl_parallel_force_inline constexpr void _unroll_impl(std::integer_sequence<T, indeces...>, F&& f) {
    (f(std::integral_constant<T, indeces>{}), ...);
}
template <class T, T count, class F>
utl_parallel_force_inline constexpr void _unroll(F&& f) {
    _unroll_impl(std::make_integer_sequence<T, count>{}, std::forward<F>(f));
}

// ===================
// --- Thread pool ---
// ===================

class ThreadPool {
private:
    std::vector<std::thread>     threads;
    mutable std::recursive_mutex thread_mutex;

    std::queue<std::packaged_task<void()>> tasks{};
    mutable std::mutex                     task_mutex;

    std::condition_variable task_cv;          // used to notify changes to the task queue
    std::condition_variable task_finished_cv; // used to notify of finished tasks

    // Signals
    bool stopping = false; // signal for workers to shut down '.worker_main()'
    bool paused   = false; // signal for workers to not pull new tasks from the queue
    bool waiting  = false; // signal for workers that they should notify 'task_finished_cv' when
                           // finishing a task, which is used to implement 'wait for tasks' methods

    int tasks_running = 0; // number of tasks currently executed by workers

    // Main function for worker threads,
    // here workers wait for the queue, pull new tasks from it and run them
    void thread_main() {
        bool task_was_finished = false;

        while (true) {
            std::unique_lock<std::mutex> task_lock(this->task_mutex);

            if (task_was_finished) {
                --this->tasks_running;
                if (this->waiting) this->task_finished_cv.notify_all();
                // no need to set 'task_was_finished' back to 'false',
                // the only way we get back into this condition is if another task was finished
            }

            // Pool isn't destructing, isn't paused and there are tasks available in the queue
            //    => continue execution, a new task from the queue and start executing it
            // otherwise
            //    => unlock the mutex and wait until a new task is submitted,
            //       pool is unpaused or destruction is initiated
            this->task_cv.wait(task_lock, [&] { return this->stopping || (!this->paused && !this->tasks.empty()); });

            if (this->stopping) break; // escape hatch for thread destruction

            // Pull a new task from the queue and start executing it
            std::packaged_task<void()> task_to_execute = std::move(this->tasks.front());
            this->tasks.pop();
            ++this->tasks_running;
            task_lock.unlock();

            task_to_execute(); // NOTE: Should I catch exceptions here?
            task_was_finished = true;
        }
    }

    void start_threads(std::size_t worker_count_increase) {
        const std::lock_guard<std::recursive_mutex> thread_lock(this->thread_mutex);
        // the mutex has to be recursive because we call '.start_threads()' inside '.set_num_threads()'
        // which also locks 'worker_mutex', if mutex wan't recursive we would deadlock trying to lock
        // it a 2nd time on the same thread.

        // NOTE: It feels like '.start_threads()' can be split into '.start_threads()' and
        // '._start_threads_assuming_locked()' which would remove the need for recursive mutex

        for (std::size_t i = 0; i < worker_count_increase; ++i)
            this->threads.emplace_back(&ThreadPool::thread_main, this);
    }

    void stop_all_threads() {
        const std::lock_guard<std::recursive_mutex> thread_lock(this->thread_mutex);

        {
            const std::lock_guard<std::mutex> task_lock(this->task_mutex);
            this->stopping = true;
            this->task_cv.notify_all();
        } // signals to all threads that they should stop running

        for (auto& worker : this->threads)
            if (worker.joinable()) worker.join();
        // 'joinable()' checks in needed so we don't try to join the master thread

        this->threads.clear();
    }

public:
    // --- Construction ---
    // --------------------

    ThreadPool() = default;

    explicit ThreadPool(std::size_t thread_count) { this->start_threads(thread_count); }

    ~ThreadPool() {
        this->unpause();
        this->wait_for_tasks();
        this->stop_all_threads();
    }

    // --- Threads ---
    // ---------------

    std::size_t get_thread_count() const {
        const std::lock_guard<std::recursive_mutex> thread_lock(this->thread_mutex);
        return this->threads.size();
    }

    void set_thread_count(std::size_t thread_count) {
        const std::size_t current_thread_count = this->get_thread_count();

        if (thread_count == current_thread_count) return;
        // 'quick escape' so we don't experience too much slowdown when the user calls '.set_thread_count()' reapeatedly

        if (thread_count > current_thread_count) {
            this->start_threads(thread_count - current_thread_count);
        } else {
            this->stop_all_threads();
            {
                const std::lock_guard<std::mutex> task_lock(this->task_mutex);
                this->stopping = false;
            }
            this->start_threads(thread_count);
            // It is possible to improve implementation by making the pool shrink by joining only the necessary amount
            // of threads instead of recreating the the whole pool, however that task is non-trivial and would likely
            // require a more granular signaling with one flag per thread instead of a global 'stopping' flag.
        }
    }

    // --- Task queue ---
    // ------------------

    template <class Func, class... Args>
    void add_task(Func&& func, Args&&... args) {
        const std::lock_guard<std::mutex> task_lock(this->task_mutex);
        this->tasks.emplace(std::bind(std::forward<Func>(func), std::forward<Args>(args)...));
        this->task_cv.notify_one(); // wakes up one thread (if possible) so it can pull the new task
    }

    template <class Func, class... Args,
              class FuncReturnType = std::invoke_result_t<std::decay_t<Func>, std::decay_t<Args>...>>
    [[nodiscard]] std::future<FuncReturnType> add_task_with_future(Func&& func, Args&&... args) {
#if defined(_MSC_VER) || defined(_MSC_FULL_VER)
        // MSVC messed up implementation of 'std::packaged_task<>' so it is not movable (which,
        // according to the standard, it should be) and they can't fix it for 7 (and counting) years
        // because fixing the bug would change the ABI. See this thread about the bug report:
        // https://developercommunity.visualstudio.com/t/unable-to-move-stdpackaged-task-into-any-stl-conta/108672
        // As a workaround we wrap the packaged task into a shared pointer and add another layer of packaging.
        auto new_task = std::make_shared<std::packaged_task<FuncReturnType()>>(
            std::bind(std::forward<Func>(func), std::forward<Args>(args)...));
        this->add_task([new_task] { (*new_task)(); }); // horrible
        return new_task->get_future();
#else
        std::packaged_task<FuncReturnType()> new_task(std::bind(std::forward<Func>(func), std::forward<Args>(args)...));
        auto                                 future = new_task.get_future();
        this->add_task(std::move(new_task));
        return future;
#endif
    }

    void wait_for_tasks() {
        std::unique_lock<std::mutex> task_lock(this->task_mutex);
        this->waiting = true;
        this->task_finished_cv.wait(task_lock, [&] { return this->tasks.empty() && this->tasks_running == 0; });
        this->waiting = false;
    }

    void clear_task_queue() {
        const std::lock_guard<std::mutex> task_lock(this->task_mutex);
        this->tasks = {}; // for some reason 'std::queue' has no '.clear()', complexity O(N)
    }

    // --- Pausing ---
    // ---------------

    void pause() {
        const std::lock_guard<std::mutex> task_lock(this->task_mutex);
        this->paused = true;
    }

    void unpause() {
        const std::lock_guard<std::mutex> task_lock(this->task_mutex);
        this->paused = false;
        this->task_cv.notify_all();
    }

    [[nodiscard]] bool is_paused() const {
        const std::lock_guard<std::mutex> task_lock(this->task_mutex);
        return this->paused;
    }
};

// =====================================
// --- Static thread pool operations ---
// =====================================

inline ThreadPool& static_thread_pool() {
    static ThreadPool pool(max_thread_count());
    return pool;
}

inline std::size_t get_thread_count() { return static_thread_pool().get_thread_count(); }

inline void set_thread_count(std::size_t thread_count) { static_thread_pool().set_thread_count(thread_count); }

// ================
// --- Task API ---
// ================

template <class Func, class... Args>
void task(Func&& func, Args&&... args) {
    static_thread_pool().add_task(std::forward<Func>(func), std::forward<Args>(args)...);
}

template <class Func, class... Args>
auto task_with_future(Func&& func, Args&&... args)
    -> std::future<std::invoke_result_t<std::decay_t<Func>, std::decay_t<Args>...>> {
    return static_thread_pool().add_task_with_future(std::forward<Func>(func), std::forward<Args>(args)...);
}

inline void wait_for_tasks() { static_thread_pool().wait_for_tasks(); }

// =======================
// --- Parallel ranges ---
// =======================

constexpr std::size_t default_grains_per_thread = 4;
// by default we distribute 4 tasks per thread, this number is purely empirical.
// We don't want to split up work into too many tasks (like with 'grain_size = 1')
// yet we want it to be a bit more granular than doing 1 task per threads since
// that would be horrible if tasks are uneven.

// Note:
// In range constructors we intentionally allow some possibly narrowing conversions like 'it1 - it2' to 'size_t'
// for better compatibility with containers that can use large ints as their difference type

template <class Idx>
struct IndexRange {
    Idx         first;
    Idx         last;
    std::size_t grain_size;

    IndexRange() = delete;
    constexpr IndexRange(Idx first, Idx last, std::size_t grain_size)
        : first(first), last(last), grain_size(grain_size) {}
    IndexRange(Idx first, Idx last)
        : IndexRange(first, last, _max_size(1, (last - first) / (get_thread_count() * default_grains_per_thread))){};
};

template <class Iter>
struct Range {
    Iter        begin;
    Iter        end;
    std::size_t grain_size;

    Range() = delete;
    constexpr Range(Iter begin, Iter end, std::size_t grain_size) : begin(begin), end(end), grain_size(grain_size) {}
    Range(Iter begin, Iter end)
        : Range(begin, end, _max_size(1, (end - begin) / (get_thread_count() * default_grains_per_thread))) {}


    template <class Container>
    Range(const Container& container) : Range(container.begin(), container.end()) {}

    template <class Container>
    Range(Container& container) : Range(container.begin(), container.end()) {}
}; // requires 'Iter' to be random-access-iterator

// User-defined deduction guides
//
// By default, template constructors cannot deduce template argument 'Iter',
// however it is possible to define a custom deduction guide and achieve what we want
//
// See: https://en.cppreference.com/w/cpp/language/class_template_argument_deduction#User-defined_deduction_guides
template <class Container>
Range(const Container& container) -> Range<typename Container::const_iterator>;

template <class Container>
Range(Container& container) -> Range<typename Container::iterator>;

// ==========================
// --- 'Parallel for' API ---
// ==========================

template <class Idx, class Func>
void for_loop(IndexRange<Idx> range, Func&& func) {
    for (Idx i = range.first; i < range.last; i += range.grain_size)
        task(std::forward<Func>(func), i, _min_size(i + range.grain_size, range.last));

    wait_for_tasks();
}

template <class Iter, class Func>
void for_loop(Range<Iter> range, Func&& func) {
    for (Iter i = range.begin; i < range.end; i += range.grain_size)
        task(std::forward<Func>(func), i, i + _min_size(range.grain_size, range.end - i));

    wait_for_tasks();
}

template <class Container, class Func>
void for_loop(const Container& container, Func&& func) {
    for_loop(Range{container}, std::forward<Func>(func));
}

template <class Container, class Func>
void for_loop(Container& container, Func&& func) {
    for_loop(Range{container}, std::forward<Func>(func));
}
// couldn't figure out how to make it work perfect-forwared 'Container&&',
// for some reason it would always cause template deduction to fail

// =============================
// --- 'Parallel reduce' API ---
// =============================

constexpr std::size_t default_unroll = 1;

template <std::size_t unroll = default_unroll, class Iter, class BinaryOp, class T = typename Iter::value_type>
auto reduce(Range<Iter> range, BinaryOp&& op) -> T {

    std::mutex result_mutex;
    T          result = *range.begin;
    // we have to start from the 1st element and not 'T{}' because there is no guarantee
    // than doing so would be correct for some non-trivial 'T' and 'op'

    for_loop(Range<Iter>{range.begin + 1, range.end, range.grain_size}, [&](Iter low, Iter high) {
        const std::size_t range_size = high - low;

        // Execute unrolled loop if unrolling is enabled and the range is sufficiently large
        if constexpr (unroll > 1)
            if (range_size > unroll) {
                // (parallel section) Compute partial result (unrolled for SIMD)
                // Reduce unrollable part
                std::array<T, unroll> partial_results;
                _unroll<std::size_t, unroll>([&](std::size_t j) { partial_results[j] = *(low + j); });
                Iter it = low + unroll;
                for (; it < high - unroll; it += unroll)
                    _unroll<std::size_t, unroll>(
                        [&, it](std::size_t j) { partial_results[j] = op(partial_results[j], *(it + j)); });
                // Reduce remaining elements
                for (; it < high; ++it) partial_results[0] = op(partial_results[0], *it);
                // Collect the result
                for (std::size_t i = 1; i < partial_results.size(); ++i)
                    partial_results[0] = op(partial_results[0], partial_results[i]);

                // (critical section) Add partial result to the global one
                const std::lock_guard<std::mutex> result_lock(result_mutex);
                result = op(result, partial_results[0]);

                return; // skip the non-unrolled version
            }

        // Fallback onto a regular reduction loop otherwise
        // (parallel section) Compute partial result
        T partial_result = *low;
        for (auto it = low + 1; it != high; ++it) partial_result = op(partial_result, *it);

        // (critical section) Add partial result to the global one
        const std::lock_guard<std::mutex> result_lock(result_mutex);
        result = op(result, partial_result);
    });

    // Note 1:
    // We could also collect results into an array of partial results and then reduce it on the
    // main thread at the end, but that leads to a much less clean implementation and doesn't
    // seem to be measurably faster.

    // Note 2:
    // 'if constexpr (unroll > 1)' ensures that unrolling logic will have no effect
    //  whatsoever on the non-unrolled version of the template, it will not even compile.

    return result;
}

template <std::size_t unroll = default_unroll, class Container, class BinaryOp>
auto reduce(Container& container, BinaryOp&& op) -> typename Container::value_type {
    return reduce<unroll>(Range{container}, std::forward<BinaryOp>(op));
}

template <std::size_t unroll = default_unroll, class Container, class BinaryOp>
auto reduce(const Container& container, BinaryOp&& op) -> typename Container::value_type {
    return reduce<unroll>(Range{container}, std::forward<BinaryOp>(op));
}

// --- Pre-defined binary ops ---
// ------------------------------

// Note:
// Defining binary operations as free-standing function has a HUGE effect on
// parallel::reduce performance, for example on 4 threads:
//    binary op is 'sum'        => speedup ~90%-180%
//    binary op is 'struct sum' => speedup ~370%
// This is caused by failed inlining and seems to be the reason why standard library implements
// 'std::plus' as a functor class and not a free-standing function. I spent 2 hours of my life
// and 4 rewrites of 'parallel::reduce()' on this. Force-inline just to be sure.

template <class T>
struct sum {
    utl_parallel_force_inline constexpr T operator()(const T& lhs, const T& rhs) const { return lhs + rhs; }
};


template <class T>
struct prod {
    utl_parallel_force_inline constexpr T operator()(const T& lhs, const T& rhs) const { return lhs * rhs; }
};

template <class T>
struct min {
    utl_parallel_force_inline constexpr T operator()(const T& lhs, const T& rhs) const {
        return (rhs < lhs) ? rhs : lhs;
    }
};

template <class T>
struct max {
    utl_parallel_force_inline constexpr T operator()(const T& lhs, const T& rhs) const {
        return (rhs < lhs) ? lhs : rhs;
    }
};

// Clean up codegen macros
#undef utl_parallel_force_inline

} // namespace utl::parallel

#endif
#endif // module utl::parallel
